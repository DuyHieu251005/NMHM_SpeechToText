\chapter{Cấu hình huấn luyện}

\section{Wav2Vec2}
\subsection{Hàm mất mát}
Sử dụng phối hợp giữa \textbf{InfoNCE Loss}, \textbf{Diversity Loss} và \textbf{CTC Loss} nhằm giải quyết nút thắt lớn nhất của Deep Learning là thiếu dữ liệu dán nhãn.

\subsubsection{InfoNCE Loss}
Được dùng trong giai đoạn \textbf{Self-Supervised Pretraining}.\\
\textbf{Tại sao dùng?} InfoNCE buộc mô hình phải hiểu nội dung cao cấp (như âm vị, ngữ điệu) để phân biệt đâu là đoạn âm thanh "đúng" và đâu là nhiễu, thay vì chỉ học vẹt các chi tiết ở mức thấp (low-level signal).

\subsubsection{Diversity Loss}
Được dùng trong giai đoạn \textbf{Self-Supervised Pretraining}.\\
\textbf{Tại sao dùng?} Để ngăn chặn hiện tượng "Codebook Collapse" (hoặc Mode Collapse).

\subsubsection{CTC Loss (Connectionist Temporal Classification)}
Được dùng trong giai đoạn \textbf{Supervised Fine-tuning}.\\
\textbf{Tại sao dùng?} Đây là tiêu chuẩn vàng cho các bài toán Sequence-to-Sequence trong ASR khi không có thông tin căn chỉnh (alignment info) từ trước.

\subsection{Thuật toán tối ưu}
\textbf{Optimizer:} AdamW (mặc định HF Trainer).\\
\textbf{Learning rate / Scheduler:} Có scheduler (warm-up lên rồi decay) 
\subsection{Siêu tham số}
\textbf{Siêu tham số}:
\begin{itemize}
    \item train\_batch\_size: 4
    \item num\_train\_epochs: 5
    \item max\_steps: 3645
    \item logging\_steps: 50
    \item save\_steps: 400
    \item eval\_steps: 200
    \item global\_step: 3645
\end{itemize}
\noindent\textbf{Phương pháp tinh chỉnh siêu tham số:} thủ công

\pagebreak
\section{PhoWhisper}
\subsection{Hàm mất mát}
Sử dụng \textbf{Cross-Entropy Loss} trên đầu ra sequence-to-sequence của Whisper; padding token được thay bằng (-100) để không tính vào loss (xem phowhisper.py).\
Đánh giá bằng \textbf{WER}.

\subsection{Thuật toán tối ưu}
\textbf{Optimizer:} AdamW (mặc định HF Trainer).\\
\textbf{Learning rate / Scheduler:} có sử dụng scheduler decay.

\subsection{Siêu tham số}
\textbf{Siêu tham số}:
\begin{itemize}
\item per\_device\_train\_batch\_size: 4
\item gradient\_accumulation\_steps: 4
\item learning\_rate: 1e-3
\item max\_steps: 500
\item fp16: True
\item save\_steps: 100, eval\_steps: 100, logging\_steps: 50
\item dataloader\_num\_workers: 0
\end{itemize}
\textbf{Tinh chỉnh (LoRA)}:
\begin{itemize}
    \item r=32
    \item lora\_alpha=64 
    \item target\_modules=["q\_proj","v\_proj"]
    \item lora\_dropout=0.05
    \item bias="none"
\end{itemize}

\noindent\textbf{Phương pháp tinh chỉnh siêu tham số:} thủ công

\pagebreak
\section{OpenAI Whisper}
\subsection{Hàm mất mát}
Sử dụng \textbf{Cross-Entropy Loss} trên đầu ra sequence-to-sequence của Whisper.\\
\textbf{Đánh giá:} WER.

\subsection{Thuật toán tối ưu}
\textbf{Optimizer:} AdamW (mặc định HF Trainer).\\
\textbf{Learning rate / Scheduler:} sử dụng warm-up lên rồi decay.

\subsection{Siêu tham số}
\textbf{Siêu tham số}:
\begin{itemize}
\item per\_device\_train\_batch\_size: 16
\item gradient\_accumulation\_steps: 1
\item learning\_rate: 1e-5
\item warmup\_steps: 100
\item num\_train\_epochs: 15
\item eval\_strategy: epoch
\item save\_strategy: epoch
\item save\_total\_limit: 2
\item max\_steps: -1 (disabled)
\item logging\_steps: 50
\item fp16: True
\item per\_device\_eval\_batch\_size: 8
\item generation\_max\_length: 225
\item report\_to: tensorboard
\item load\_best\_model\_at\_end: True
\item metric\_for\_best\_model: wer
\item greater\_is\_better: False
\item dataloader\_num\_workers: 2
\item dataloader\_pin\_memory: True
\item remove\_unused\_columns: False
\end{itemize}

\noindent\textbf{Tinh chỉnh siêu tham số:} thủ công.