"""
Script Ä‘Ã¡nh giÃ¡ model PhoWhisper - Chá»‰ táº¡o 2 hÃ¬nh:
1. Learning Curves (Loss, WER)
2. Error Distribution
"""

import torch
import json
import matplotlib.pyplot as plt
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel
import librosa
import os
from tqdm import tqdm
import evaluate
from pathlib import Path

# ==========================================
# 1. LOAD TRAINING HISTORY
# ==========================================
def load_training_history(checkpoint_path):
    """Äá»c trainer_state.json Ä‘á»ƒ láº¥y lá»‹ch sá»­ training"""
    trainer_state_path = os.path.join(checkpoint_path, "trainer_state.json")
    
    with open(trainer_state_path, 'r', encoding='utf-8') as f:
        trainer_state = json.load(f)
    
    log_history = trainer_state['log_history']
    
    # TÃ¡ch ra train loss vÃ  eval metrics
    train_logs = []
    eval_logs = []
    
    for log in log_history:
        if 'loss' in log and 'eval_loss' not in log:
            train_logs.append({
                'epoch': log['epoch'],
                'loss': log['loss'],
                'step': log['step']
            })
        elif 'eval_loss' in log:
            eval_logs.append({
                'epoch': log['epoch'],
                'eval_loss': log['eval_loss'],
                'eval_wer': log.get('eval_wer', None),
                'step': log['step']
            })
    
    return train_logs, eval_logs

# ==========================================
# 2. Váº¼ LEARNING CURVES
# ==========================================
def plot_learning_curves(train_logs, eval_logs, save_path='phowhisper_learning_curves.png'):
    """Váº½ biá»ƒu Ä‘á»“ Loss vÃ  WER theo epoch"""
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # Subplot 1: Training Loss vs Validation Loss
    train_epochs = [log['epoch'] for log in train_logs]
    train_losses = [log['loss'] for log in train_logs]
    
    eval_epochs = [log['epoch'] for log in eval_logs]
    eval_losses = [log['eval_loss'] for log in eval_logs]
    
    axes[0].plot(train_epochs, train_losses, 'b-', label='Training Loss', alpha=0.7)
    axes[0].plot(eval_epochs, eval_losses, 'r-', label='Validation Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Subplot 2: Validation WER
    eval_wers = [log['eval_wer'] for log in eval_logs if log['eval_wer'] is not None]
    
    axes[1].plot(eval_epochs[:len(eval_wers)], eval_wers, 'g-o', label='Validation WER', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('WER (%)', fontsize=12)
    axes[1].set_title('Word Error Rate on Validation Set', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ÄÃ£ lÆ°u learning curves vÃ o: {save_path}")
    
    # Nháº­n xÃ©t
    print("\n" + "="*80)
    print("NHáº¬N XÃ‰T Vá»€ BIá»‚U Äá»’ LEARNING CURVES:")
    print("="*80)
    
    # Kiá»ƒm tra há»™i tá»¥
    if len(train_losses) > 10:
        recent_train_loss = np.mean(train_losses[-5:])
        initial_train_loss = np.mean(train_losses[:5])
        improvement = ((initial_train_loss - recent_train_loss) / initial_train_loss) * 100
        
        print(f"ğŸ“Š Training Loss giáº£m tá»« {initial_train_loss:.4f} â†’ {recent_train_loss:.4f} ({improvement:.1f}% cáº£i thiá»‡n)")
    
    if len(eval_losses) > 2:
        recent_eval_loss = np.mean(eval_losses[-2:])
        initial_eval_loss = eval_losses[0]
        print(f"ğŸ“Š Validation Loss giáº£m tá»« {initial_eval_loss:.4f} â†’ {recent_eval_loss:.4f}")
    
    # Kiá»ƒm tra dao Ä‘á»™ng
    if len(train_losses) > 10:
        train_std = np.std(train_losses[-10:])
        print(f"ğŸ“Š Äá»™ dao Ä‘á»™ng Training Loss (10 epochs cuá»‘i): {train_std:.4f}")
        if train_std > 0.5:
            print("âš ï¸  MÃ´ hÃ¬nh cÃ³ dao Ä‘á»™ng máº¡nh")
        else:
            print("âœ… MÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh")
    
    # Kiá»ƒm tra overfitting
    if len(eval_losses) > 2 and len(train_losses) > 2:
        final_gap = eval_losses[-1] - train_losses[-1]
        print(f"ğŸ“Š Khoáº£ng cÃ¡ch Train-Validation Loss cuá»‘i: {final_gap:.4f}")
        if final_gap > 0.5:
            print("âš ï¸  CÃ³ dáº¥u hiá»‡u Overfitting")
        else:
            print("âœ… KhÃ´ng cÃ³ dáº¥u hiá»‡u overfitting nghiÃªm trá»ng")
    
    print("="*80 + "\n")

# ==========================================
# 3. ÄÃNH GIÃ MODEL TRÃŠN TEST SET
# ==========================================
def evaluate_model_on_test_set(model, processor, test_data_path, max_samples=200):
    """ÄÃ¡nh giÃ¡ model trÃªn test set cá»§a VIVOS"""
    
    wer_metric = evaluate.load("wer")
    cer_metric = evaluate.load("cer")
    
    # Load test data
    prompts_path = os.path.join(test_data_path, "prompts.txt")
    waves_dir = os.path.join(test_data_path, "waves")
    
    with open(prompts_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    references = []
    predictions = []
    individual_wers = []
    
    print(f"\nğŸ” Äang Ä‘Ã¡nh giÃ¡ trÃªn {min(len(lines), max_samples)} máº«u test...")
    
    model.eval()
    
    for i, line in enumerate(tqdm(lines[:max_samples])):
        parts = line.strip().split(" ", 1)
        if len(parts) != 2:
            continue
            
        file_id, reference_text = parts
        speaker_id = file_id.split("_")[0]
        audio_path = os.path.join(waves_dir, speaker_id, f"{file_id}.wav")
        
        if not os.path.exists(audio_path):
            continue
        
        try:
            # Load audio
            audio, rate = librosa.load(audio_path, sr=16000)
            
            # Predict
            input_features = processor(
                audio, 
                sampling_rate=16000, 
                return_tensors="pt"
            ).input_features.to(model.device)
            
            with torch.no_grad():
                predicted_ids = model.generate(
                    input_features,
                    max_new_tokens=100,
                    no_repeat_ngram_size=3,
                    repetition_penalty=1.2,
                    language="vietnamese"
                )
            
            prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            
            ref_text = reference_text.lower()
            pred_text = prediction.lower()
            
            references.append(ref_text)
            predictions.append(pred_text)
            
            # TÃ­nh WER cho tá»«ng máº«u
            try:
                wer_single = wer_metric.compute(predictions=[pred_text], references=[ref_text])
                individual_wers.append(wer_single * 100)
            except:
                individual_wers.append(0)
            
        except Exception as e:
            continue
    
    # TÃ­nh metrics tá»•ng thá»ƒ
    wer_score = wer_metric.compute(predictions=predictions, references=references)
    cer_score = cer_metric.compute(predictions=predictions, references=references)
    
    print("\n" + "="*80)
    print("Káº¾T QUáº¢ ÄÃNH GIÃ TRÃŠN TEST SET:")
    print("="*80)
    print(f"ğŸ“Š WER (Word Error Rate): {wer_score*100:.2f}%")
    print(f"ğŸ“Š CER (Character Error Rate): {cer_score*100:.2f}%")
    print(f"ğŸ“Š Sá»‘ máº«u Ä‘Ã¡nh giÃ¡: {len(predictions)}")
    print("="*80 + "\n")
    
    # In má»™t vÃ i vÃ­ dá»¥
    print("\nğŸ” Má»˜T Sá» VÃ Dá»¤ Dá»° ÄOÃN:")
    print("-"*80)
    for i in range(min(5, len(predictions))):
        print(f"\nMáº«u {i+1}:")
        print(f"  Reference:  {references[i]}")
        print(f"  Prediction: {predictions[i]}")
    print("-"*80 + "\n")
    
    return {
        'wer': wer_score * 100,
        'cer': cer_score * 100,
        'num_samples': len(predictions),
        'individual_wers': individual_wers
    }

# ==========================================
# 4. Váº¼ ERROR DISTRIBUTION
# ==========================================
def plot_error_distribution(individual_wers, overall_wer, save_path='phowhisper_error_distribution.png'):
    """Váº½ phÃ¢n phá»‘i lá»—i WER trÃªn cÃ¡c máº«u test"""
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # Subplot 1: Histogram cá»§a WER
    axes[0].hist(individual_wers, bins=30, color='#3498db', alpha=0.7, edgecolor='black')
    axes[0].axvline(overall_wer, color='red', linestyle='--', linewidth=2, label=f'Mean WER: {overall_wer:.2f}%')
    axes[0].set_xlabel('WER (%)', fontsize=12)
    axes[0].set_ylabel('Sá»‘ lÆ°á»£ng máº«u', fontsize=12)
    axes[0].set_title('PhÃ¢n Phá»‘i Word Error Rate', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # Subplot 2: PhÃ¢n loáº¡i máº«u theo Ä‘á»™ chÃ­nh xÃ¡c
    wer_categories = {
        'Xuáº¥t sáº¯c\n(WER < 20%)': sum(1 for w in individual_wers if w < 20),
        'Tá»‘t\n(20% â‰¤ WER < 40%)': sum(1 for w in individual_wers if 20 <= w < 40),
        'Trung bÃ¬nh\n(40% â‰¤ WER < 60%)': sum(1 for w in individual_wers if 40 <= w < 60),
        'KÃ©m\n(WER â‰¥ 60%)': sum(1 for w in individual_wers if w >= 60)
    }
    
    categories = list(wer_categories.keys())
    counts = list(wer_categories.values())
    colors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']
    
    bars = axes[1].bar(range(len(categories)), counts, color=colors, alpha=0.7, edgecolor='black')
    axes[1].set_xticks(range(len(categories)))
    axes[1].set_xticklabels(categories, fontsize=10)
    axes[1].set_ylabel('Sá»‘ lÆ°á»£ng máº«u', fontsize=12)
    axes[1].set_title('PhÃ¢n Loáº¡i Máº«u Theo Äá»™ ChÃ­nh XÃ¡c', fontsize=14, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='y')
    
    # ThÃªm sá»‘ lÆ°á»£ng lÃªn tá»«ng cá»™t
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{count}\n({count/len(individual_wers)*100:.1f}%)',
                    ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ÄÃ£ lÆ°u error distribution vÃ o: {save_path}")
    
    # PhÃ¢n tÃ­ch chi tiáº¿t
    print("\n" + "="*80)
    print("PHÃ‚N TÃCH ERROR DISTRIBUTION:")
    print("="*80)
    print(f"ğŸ“Š WER trung bÃ¬nh: {overall_wer:.2f}%")
    print(f"ğŸ“Š WER tháº¥p nháº¥t: {min(individual_wers):.2f}%")
    print(f"ğŸ“Š WER cao nháº¥t: {max(individual_wers):.2f}%")
    print(f"ğŸ“Š Äá»™ lá»‡ch chuáº©n: {np.std(individual_wers):.2f}%")
    print(f"ğŸ“Š Trung vá»‹ (Median): {np.median(individual_wers):.2f}%")
    print("\nğŸ“Š PhÃ¢n loáº¡i máº«u:")
    for category, count in wer_categories.items():
        percentage = (count / len(individual_wers)) * 100
        print(f"   {category.replace(chr(10), ' ')}: {count} máº«u ({percentage:.1f}%)")
    print("="*80 + "\n")

# ==========================================
# 5. MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    print("="*80)
    print("ÄÃNH GIÃ MODEL PHOWHISPER - Táº O 2 HÃŒNH")
    print("="*80 + "\n")
    
    # Cáº¥u hÃ¬nh
    CHECKPOINT_PATH = "./phowhisper-finetuned-local/checkpoint-500"
    BASE_MODEL_ID = "vinai/PhoWhisper-small"
    TEST_DATA_PATH = r"C:\Users\phamm\Downloads\Compressed\archive\vivos\test"
    
    # 1. Váº½ Learning Curves
    print("ğŸ“Š BÆ°á»›c 1/4: Váº½ Learning Curves...")
    train_logs, eval_logs = load_training_history(CHECKPOINT_PATH)
    plot_learning_curves(train_logs, eval_logs)
    
    # 2. Load model
    print("\nğŸ“¦ BÆ°á»›c 2/4: Load model...")
    processor = WhisperProcessor.from_pretrained(
        BASE_MODEL_ID, 
        language="vietnamese", 
        task="transcribe"
    )
    model = WhisperForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID, 
        device_map="auto"
    )
    model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)
    
    # 3. ÄÃ¡nh giÃ¡ trÃªn test set
    print("\nğŸ“Š BÆ°á»›c 3/4: ÄÃ¡nh giÃ¡ trÃªn test set...")
    if os.path.exists(TEST_DATA_PATH):
        results = evaluate_model_on_test_set(model, processor, TEST_DATA_PATH, max_samples=200)
        
        # 4. Váº½ Error Distribution
        print("\nğŸ“Š BÆ°á»›c 4/4: Váº½ Error Distribution...")
        plot_error_distribution(results['individual_wers'], results['wer'])
        
        # LÆ°u káº¿t quáº£
        with open('phowhisper_evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'wer': results['wer'],
                'cer': results['cer'],
                'num_samples': results['num_samples']
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o: phowhisper_evaluation_results.json")
        
        print("\n" + "="*80)
        print("âœ… Káº¾T QUáº¢ CUá»I CÃ™NG:")
        print("="*80)
        print(f"ğŸ“Š ÄÃ£ táº¡o 2 hÃ¬nh:")
        print(f"   1. phowhisper_learning_curves.png")
        print(f"   2. phowhisper_error_distribution.png")
        print(f"\nğŸ“Š File káº¿t quáº£: phowhisper_evaluation_results.json")
        print("="*80)
    else:
        print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y test data táº¡i: {TEST_DATA_PATH}")
    
    print("\nâœ… HOÃ€N THÃ€NH!")
