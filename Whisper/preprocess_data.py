"""
Tiá»n xá»­ lÃ½ dá»¯ liá»‡u VIVOS cho Vietnamese Speech Recognition
Äá»“ Ã¡n Nháº­p mÃ´n Há»c mÃ¡y - HCMUS

File nÃ y thá»±c hiá»‡n:
1. Load dá»¯ liá»‡u VIVOS tá»« á»• cá»©ng
2. Tiá»n xá»­ lÃ½ audio (resampling 16kHz)
3. Chuáº©n hÃ³a text (lowercase)
4. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (Log-Mel Spectrogram cho Whisper)
5. Tokenization
6. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ sá»­ dá»¥ng cho training
"""

import os
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass
from tqdm import tqdm

# Audio processing
import librosa
import soundfile as sf

# Dataset handling
from datasets import Dataset, DatasetDict, Audio

# Transformers
from transformers import WhisperProcessor, WhisperFeatureExtractor


# ==================== Cáº¤U HÃŒNH ====================
@dataclass
class DataConfig:
    """Cáº¥u hÃ¬nh cho tiá»n xá»­ lÃ½ dá»¯ liá»‡u"""
    # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c VIVOS
    data_path: str = r"D:\Data\vivos"  # Thay Ä‘á»•i theo Ä‘Æ°á»ng dáº«n cá»§a báº¡n
    
    # ThÃ´ng sá»‘ audio
    sample_rate: int = 16000  # Táº§n sá»‘ láº¥y máº«u chuáº©n cho ASR
    max_duration: float = 30.0  # Äá»™ dÃ i tá»‘i Ä‘a audio (giÃ¢y)
    min_duration: float = 0.5  # Äá»™ dÃ i tá»‘i thiá»ƒu audio (giÃ¢y)
    
    # ThÃ´ng sá»‘ xá»­ lÃ½
    normalize_text: bool = True  # Chuáº©n hÃ³a text vá» lowercase
    remove_punctuation: bool = False  # Giá»¯ láº¡i dáº¥u cÃ¢u
    
    # MÃ´ hÃ¬nh processor
    whisper_model: str = "openai/whisper-small"
    
    # ThÆ° má»¥c output
    output_dir: str = "./processed_data"
    
    # Cache
    use_cache: bool = True


# ==================== HÃ€M TIá»€N Xá»¬ LÃ ====================

def normalize_vietnamese_text(text: str) -> str:
    """
    Chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t
    - Chuyá»ƒn vá» chá»¯ thÆ°á»ng
    - Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    """
    # Chuyá»ƒn vá» lowercase
    text = text.lower().strip()
    
    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    text = ' '.join(text.split())
    
    return text


def load_audio(audio_path: str, target_sr: int = 16000) -> Optional[np.ndarray]:
    """
    Load vÃ  resample audio vá» táº§n sá»‘ má»¥c tiÃªu
    
    Args:
        audio_path: ÄÆ°á»ng dáº«n file audio
        target_sr: Táº§n sá»‘ láº¥y máº«u má»¥c tiÃªu (máº·c Ä‘á»‹nh 16kHz)
    
    Returns:
        Numpy array chá»©a audio waveform hoáº·c None náº¿u lá»—i
    """
    try:
        # Load audio vá»›i librosa
        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)
        return audio
    except Exception as e:
        print(f"Lá»—i load audio {audio_path}: {e}")
        return None


def get_audio_duration(audio: np.ndarray, sr: int = 16000) -> float:
    """TÃ­nh Ä‘á»™ dÃ i audio (giÃ¢y)"""
    return len(audio) / sr


def validate_audio(audio: np.ndarray, config: DataConfig) -> bool:
    """
    Kiá»ƒm tra audio cÃ³ há»£p lá»‡ khÃ´ng
    - KhÃ´ng quÃ¡ ngáº¯n hoáº·c quÃ¡ dÃ i
    - KhÃ´ng pháº£i audio im láº·ng
    """
    duration = get_audio_duration(audio, config.sample_rate)
    
    # Kiá»ƒm tra Ä‘á»™ dÃ i
    if duration < config.min_duration or duration > config.max_duration:
        return False
    
    # Kiá»ƒm tra cÃ³ Ã¢m thanh khÃ´ng (RMS > threshold)
    rms = np.sqrt(np.mean(audio ** 2))
    if rms < 0.001:  # NgÆ°á»¡ng cho audio im láº·ng
        return False
    
    return True


def load_vivos_dataset(data_path: str) -> DatasetDict:
    """
    Load bá»™ dá»¯ liá»‡u VIVOS tá»« á»• cá»©ng
    
    Cáº¥u trÃºc thÆ° má»¥c VIVOS:
    vivos/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ prompts.txt
    â”‚   â””â”€â”€ waves/
    â”‚       â””â”€â”€ VIVOSSPK01/
    â”‚           â””â”€â”€ VIVOSSPK01_001.wav
    â””â”€â”€ test/
        â”œâ”€â”€ prompts.txt
        â””â”€â”€ waves/
    """
    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {data_path}")
    
    def load_split(split_name: str) -> Dataset:
        """Load má»™t split (train hoáº·c test)"""
        prompts_path = os.path.join(data_path, split_name, "prompts.txt")
        waves_path = os.path.join(data_path, split_name, "waves")
        
        if not os.path.exists(prompts_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {prompts_path}")
        
        data = []
        skipped = 0
        
        with open(prompts_path, encoding="utf-8") as f:
            for line in tqdm(f, desc=f"Loading {split_name}"):
                line = line.strip()
                if not line:
                    continue
                    
                parts = line.split(" ", 1)
                if len(parts) != 2:
                    skipped += 1
                    continue
                
                file_id, text = parts
                speaker_id = file_id.split("_")[0]
                audio_file = os.path.join(waves_path, speaker_id, f"{file_id}.wav")
                
                if os.path.exists(audio_file):
                    data.append({
                        "file_id": file_id,
                        "speaker_id": speaker_id,
                        "audio": audio_file,
                        "sentence": text
                    })
                else:
                    skipped += 1
        
        print(f"  âœ… Loaded {len(data)} samples, skipped {skipped}")
        
        # Táº¡o Dataset vÃ  cast audio column
        dataset = Dataset.from_list(data)
        dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
        
        return dataset
    
    return DatasetDict({
        "train": load_split("train"),
        "test": load_split("test")
    })


def prepare_features_whisper(
    batch: Dict,
    processor: WhisperProcessor,
    config: DataConfig
) -> Dict:
    """
    Chuáº©n bá»‹ features cho Whisper model
    
    Thá»±c hiá»‡n:
    1. TrÃ­ch xuáº¥t Log-Mel Spectrogram tá»« audio
    2. Tokenize text thÃ nh labels
    """
    audio = batch["audio"]
    
    # TrÃ­ch xuáº¥t Log-Mel Spectrogram
    input_features = processor.feature_extractor(
        audio["array"],
        sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    
    # Chuáº©n hÃ³a text
    text = batch["sentence"]
    if config.normalize_text:
        text = normalize_vietnamese_text(text)
    
    # Tokenize text
    labels = processor.tokenizer(text).input_ids
    
    return {
        "input_features": input_features,
        "labels": labels,
        "text": text
    }


def preprocess_dataset(
    dataset: DatasetDict,
    config: DataConfig,
    save: bool = True
) -> DatasetDict:
    """
    Pipeline tiá»n xá»­ lÃ½ hoÃ n chá»‰nh
    """
    print("\nğŸ”„ Äang khá»Ÿi táº¡o Processor...")
    processor = WhisperProcessor.from_pretrained(
        config.whisper_model,
        language="vietnamese",
        task="transcribe"
    )
    
    print("\nğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    
    def prepare_fn(batch):
        return prepare_features_whisper(batch, processor, config)
    
    processed_dataset = DatasetDict()
    
    for split in ["train", "test"]:
        print(f"\n  ğŸ“Š Xá»­ lÃ½ {split} set ({len(dataset[split])} samples)...")
        processed = dataset[split].map(
            prepare_fn,
            remove_columns=dataset[split].column_names,
            desc=f"Processing {split}"
        )
        processed_dataset[split] = processed
    
    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    if save:
        output_path = Path(config.output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ Äang lÆ°u dá»¯ liá»‡u vÃ o {config.output_dir}...")
        processed_dataset.save_to_disk(str(output_path / "vivos_processed"))
        
        # LÆ°u config
        config_dict = {
            "data_path": config.data_path,
            "sample_rate": config.sample_rate,
            "whisper_model": config.whisper_model,
            "train_samples": len(processed_dataset["train"]),
            "test_samples": len(processed_dataset["test"])
        }
        with open(output_path / "config.json", "w", encoding="utf-8") as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print("  âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng!")
    
    return processed_dataset


def get_dataset_statistics(dataset: DatasetDict) -> Dict:
    """
    TÃ­nh toÃ¡n cÃ¡c thá»‘ng kÃª vá» dataset
    """
    stats = {}
    
    for split in ["train", "test"]:
        data = dataset[split]
        
        # Thá»‘ng kÃª text
        texts = [item["sentence"] for item in data]
        text_lengths = [len(t.split()) for t in texts]
        
        stats[split] = {
            "num_samples": len(data),
            "avg_text_length": np.mean(text_lengths),
            "min_text_length": min(text_lengths),
            "max_text_length": max(text_lengths),
        }
    
    return stats


# ==================== MAIN ====================

def main():
    """HÃ m chÃ­nh cháº¡y tiá»n xá»­ lÃ½"""
    print("=" * 60)
    print("ğŸ¤ TIá»€N Xá»¬ LÃ Dá»® LIá»†U VIVOS CHO WHISPER")
    print("=" * 60)
    
    # Cáº¥u hÃ¬nh - Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n phÃ¹ há»£p
    config = DataConfig(
        data_path=r"D:\Data\vivos",  # âš ï¸ Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n nÃ y
        whisper_model="openai/whisper-small",
        output_dir="./processed_data"
    )
    
    # Kiá»ƒm tra Ä‘Æ°á»ng dáº«n
    if not os.path.exists(config.data_path):
        print(f"\nâŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c dá»¯ liá»‡u: {config.data_path}")
        print("Vui lÃ²ng thay Ä‘á»•i Ä‘Æ°á»ng dáº«n trong DataConfig!")
        return
    
    # Load dataset
    print("\n" + "=" * 40)
    print("ğŸ“¥ BÆ¯á»šC 1: LOAD Dá»® LIá»†U")
    print("=" * 40)
    
    dataset = load_vivos_dataset(config.data_path)
    
    # Thá»‘ng kÃª
    print("\n" + "=" * 40)
    print("ğŸ“Š BÆ¯á»šC 2: THá»NG KÃŠ Dá»® LIá»†U")
    print("=" * 40)
    
    stats = get_dataset_statistics(dataset)
    for split, split_stats in stats.items():
        print(f"\n{split.upper()}:")
        for key, value in split_stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
    
    # Tiá»n xá»­ lÃ½
    print("\n" + "=" * 40)
    print("âš™ï¸ BÆ¯á»šC 3: TIá»€N Xá»¬ LÃ")
    print("=" * 40)
    
    processed = preprocess_dataset(dataset, config, save=True)
    
    print("\n" + "=" * 60)
    print("âœ… HOÃ€N THÃ€NH!")
    print(f"ğŸ“ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {config.output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
