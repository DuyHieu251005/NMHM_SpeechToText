import torch
import librosa
import pandas as pd
import os
import evaluate
from transformers import (
    Wav2Vec2ForCTC, 
    Wav2Vec2Processor, 
    Wav2Vec2CTCTokenizer, 
    Wav2Vec2FeatureExtractor
)
from tqdm import tqdm
import unicodedata

# ==========================================
# 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN (GIá»® NGUYÃŠN NHÆ¯ CÅ¨)
# ==========================================
model_path = r"checkpoint-3645"
vivos_test_path = r"C:\Users\phamm\Downloads\Compressed\archive\vivos\test"
report_path = r"Ket_Qua_Danh_Gia.csv"

# ==========================================
# 2. HÃ€M CHUáº¨N Bá»Š Dá»® LIá»†U
# ==========================================
def load_vivos_test_data(root_path):
    prompts_path = os.path.join(root_path, "prompts.txt")
    waves_dir = os.path.join(root_path, "waves")
    
    if not os.path.exists(prompts_path):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {prompts_path}")

    with open(prompts_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    data = []
    print("â³ Äang quÃ©t file audio...")
    for line in lines:
        parts = line.strip().split(" ", 1)
        if len(parts) == 2:
            file_id, text = parts
            speaker_id = file_id.split("_")[0]
            # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file wav
            full_path = os.path.join(waves_dir, speaker_id, f"{file_id}.wav")
            
            if os.path.exists(full_path):
                data.append({"path": full_path, "text": text})
            else:
                pass # Bá» qua cáº£nh bÃ¡o cho gá»n mÃ n hÃ¬nh
    
    return data

# ==========================================
# 3. LOAD MODEL & METRIC (ÄÃƒ Sá»¬A Lá»–I)
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸  Äang cháº¡y trÃªn thiáº¿t bá»‹: {device}")

print("â³ Äang load model...")
try:
    # --- Sá»¬A Lá»–I 1: Load Tokenizer tá»« Local (Ä‘á»ƒ láº¥y Vocab cá»§a báº¡n) ---
    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(
        model_path, 
        unk_token="[UNK]", 
        pad_token="[PAD]", 
        word_delimiter_token="|"
    )

    # --- Sá»¬A Lá»–I 2: Load Feature Extractor tá»« Online (Fix lá»—i thiáº¿u file config) ---
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("nguyenvulebinh/wav2vec2-base-vietnamese-250h")

    # Gá»™p láº¡i thÃ nh Processor
    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)

    # Load Model Weights
    model = Wav2Vec2ForCTC.from_pretrained(model_path).to(device)
    print("âœ… Load model thÃ nh cÃ´ng!")

except Exception as e:
    print(f"âŒ Lá»—i load model: {e}")
    print("ğŸ‘‰ HÃ£y cháº¯c cháº¯n trong folder model cÃ³ file 'vocab.json', 'config.json', 'model.safetensors' (hoáº·c pytorch_model.bin)")
    exit()

wer_metric = evaluate.load("wer")

# ==========================================
# 4. Báº®T Äáº¦U ÄÃNH GIÃ
# ==========================================
dataset = load_vivos_test_data(vivos_test_path)
print(f"âœ… TÃ¬m tháº¥y {len(dataset)} máº«u kiá»ƒm thá»­.")

references = []
predictions = []

print("ğŸš€ Báº¯t Ä‘áº§u cháº¡y test (Viá»‡c nÃ y sáº½ máº¥t vÃ i phÃºt)...")

for item in tqdm(dataset):
    # 1. Load Audio
    speech, sr = librosa.load(item["path"], sr=16000) 
    
    # 2. Xá»­ lÃ½ input
    input_values = processor(speech, sampling_rate=16000, return_tensors="pt", padding=True).input_values.to(device)
    
    # 3. Dá»± Ä‘oÃ¡n
    with torch.no_grad():
        logits = model(input_values).logits
    
    # 4. Decode ra chá»¯
    pred_ids = torch.argmax(logits, dim=-1)
    
    # --- Sá»¬A Lá»–I 3: ThÃªm skip_special_tokens=True Ä‘á»ƒ xÃ³a [PAD] ---
    transcription = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]
    
    # 5. LÆ°u láº¡i
    ref_norm = item["text"].lower().strip()
    pred_norm = transcription.lower().strip()
    
    predictions.append(pred_norm)
    references.append(ref_norm)

# ==========================================
# 5. TÃNH ÄIá»‚M VÃ€ LÆ¯U BÃO CÃO
# ==========================================
print("\nğŸ“Š Äang tÃ­nh toÃ¡n WER...")
wer_score = wer_metric.compute(predictions=predictions, references=references)

print("="*40)
print(f"ğŸ† Káº¾T QUáº¢ CUá»I CÃ™NG:")
print(f"ğŸ‘‰ WER (Tá»· lá»‡ lá»—i): {wer_score * 100:.2f}%")
print(f"ğŸ‘‰ Äá»™ chÃ­nh xÃ¡c (Accuracy): {(1 - wer_score) * 100:.2f}%")
print("="*40)

# LÆ°u file Excel
df = pd.DataFrame({
    "Audio Path": [d['path'] for d in dataset],
    "Gá»‘c (Reference)": references,
    "MÃ¡y Ä‘oÃ¡n (Prediction)": predictions
})

df.to_csv(report_path, index=False, encoding='utf-8-sig')
print(f"âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o chi tiáº¿t táº¡i: {report_path}")